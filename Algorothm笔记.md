#Algorothm
<hr>


* 监督学习 

在拥有特征与正确结果相对应的数据情况下进行学习

* 无监督学习

聚类

### 线性回归 

* 单变量线性回归

基于数据 训练方程参数

构建cost function 计算真实值与预测值误差 得到使方程的代价最小的参数

计算cost 方法 
batch批量梯度下降 计算下降最快的方向 求导 每次进行参数更新 减去学习率*cost的导数 
学习率为下降的步长

* 多变量线性回归

特征为多维  

 处理数据尺度不同 - 特征缩放 能够提高梯度下降速度 椭圆变正圆

正规方程法（最小二乘）
>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的

![截屏2020-03-01下午6.08.03](https://tva1.sinaimg.cn/large/00831rSTly1gcevmq8btoj31820dqn00.jpg)

即：均方误差最小化
单元线性 

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevmb5f6qj30g8078my1.jpg" alt="截屏2020-03-02上午12.07.23" style="zoom:50%;" />

多元线性转为矩阵

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevmkmquqj30fu030aac.jpg" alt="截屏2020-03-02上午12.07.47" style="zoom:50%;" />
**w为（w:b）**
当矩阵为可逆时得到结果

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevsfav04j309u02qdfx.jpg" alt="截屏2020-03-02上午12.14.02" style="zoom:50%;" />



### 逻辑回归

分类问题 与预测值为离散值
sigmoid function逻辑回归
```1 / (1 + np.exp(-z))```

cost function构建

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcftenhml2j30ey02gwen.jpg" alt="截屏2020-03-02下午7.37.16" style="zoom:50%;" />
其中，<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcftg9fygyj30ev01t3yi.jpg" alt="54249cb51f0086fa6a805291bf2639f1" style="zoom:50%;" />

代入后得到最终代价函数

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcfthqgarqj30sy02m0t3.jpg" alt="截屏2020-03-02下午7.39.55" style="zoom:50%;" />
然后进行梯度下降算法求θ

逻辑回归的代价函数看起来和线性回归一样  但是 h(theta) 不同 即假设函数。线性回归多为多项式

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgt7vb51lj307i022weh.jpg" alt="截屏2020-03-03下午4.15.28" style="zoom:50%;" />



g为sigmoid X为特征向量 theta参数

一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。有：**共轭梯度**（**Conjugate Gradient**），**局部优化法**(**Broyden fletcher goldfarb shann,BFGS**)和**有限内存局部优化法**(**LBFGS**) ，**fminunc**



**共轭梯度**

**局部优化法**

**有限内存局部优化法**





### 正则化

处理过拟合问题

保留所有的特征，但是减少参数的大小
高次项导致了过拟合的产生，降低高次项系数
**cost function**

引入正则化参数lamda
* 正则化线性回归

**梯度下降方法**
<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgqzagzrhj30iq03caab.jpg" alt="截屏2020-03-03下午2.58.04" style="zoom:50%;" />

求导整理后得



<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgrt7hxlxj30k202ujrm.jpg" alt="截屏2020-03-03下午3.27.40" style="zoom:50%;" />

**正规方程**

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgrul7l50j308c0230so.jpg" alt="71d723ddb5863c943fcd4e6951114ee3" style="zoom:50%;" />


* 正则化逻辑回归

  cost function

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgsfdfui8j30xc03wmxn.jpg" alt="截屏2020-03-03下午3.48.50" style="zoom:50%;" />



* L1正则化：Lasso回归
  产生**稀疏权值矩阵**，即产生一个稀疏模型，可以用于特征选择
  权值向量w中各个元素的绝对值之和

  <img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgv4zgwcuj30ay03yaa3.jpg" alt="截屏2020-03-03下午5.22.29" style="zoom:50%;" />

令L=正则项，此时我们的任务变成在L约束下求出J0取最小值的解。

因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），J0与这些角接触的机率会远大于与LLL其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch1u29ijdj308u089js0.jpg" alt="20160904184428459" style="zoom:50%;" />



正则参数lamda 越大，越容易在x=0处取得最小值


* L2正则化：Ridge回归
可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合
权值向量w中各个元素的平方和然后再求平方根,L2损失函数：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch1uytqgtj309c032glm.jpg" alt="截屏2020-03-03下午9.15.12" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch20l6e65j308u089q3l.jpg" alt="20160904184646963" style="zoom:50%;" />

拟合过程中通常都倾向于让权值尽可能小，适应不同的数据集，也在一定程度上避免了过拟合现象，抗扰动能力强

L2正则化的梯度下降公示：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch24ehzh7j30mc03u3yv.jpg" alt="截屏2020-03-03下午9.23.52" style="zoom:50%;" />

比正常回归的多的部分（其中lamda为正则系数）

正则参数lamda 越大，theta 衰减越快，求costfunction最值时参数也会越小

#### L1、L2比较分析

L1-norm(范数)，也叫作最小绝对偏差（leastabsolute deviations, LAD），最小绝对误差（least absolute errors，LAE）.它是最小化目标值yi和估计值f(xi)绝对差值的求和。

L2-norm(范数)也称为最小均方(least squares)，它是最小化目标值yi和估计值f(xi)平方和。

**鲁棒性（Robustness）：**最小绝对值偏差的方法应用领域很广，相比最小均方的方法，它的鲁棒性更好，LAD能对数据中的异常点有很好的**抗干扰能力**，异常点可以安全的和高效的忽略，这对研究帮助很大。
如果异常值对研究很重要，最小均方误差则是更好的选择。

对于L2-norm，由于是均方误差，如果误差>1的话，那么平方后，相比L-norm而言，误差就会被放大很多。因此模型会对样例更敏感。如果样例是一个异常值，模型会调整最小化异常值的情况，以牺牲其它更一般样例为代价，因为相比单个异常样例，那些一般的样例会得到更小的损失误差。

**稳定性：**LAD方法的不稳定属性意思是，对于一个书评调整的数据集，回归线可能会跳跃很大

![20170920234145310](https://tva1.sinaimg.cn/large/00831rSTly1gch65880lnj30o109k76i.jpg)

这里的稳定性指的是对于 特征点的微调所导致的回归线的变动，微调之后 以及大调之后回归变化，所以l1比l2在此方面更具有稳定性。

**解决方案唯一性** 对于一个问题的手链方式l2只有一种最短，l1路径长度相同情况下手链方式有可能不同

![20170920234250809](https://tva1.sinaimg.cn/large/00831rSTly1gch69p8ar6j30ao0asq46.jpg)

**内置的特征选择（Built-in feature selection）**：这是L1-norm经常被提及的一个优点，而L2-norm没有。这实际上是L1-norm的一个结果，L1-norm往往会使系数变得**稀疏**（sparse coefficients），易于进行特征选择。假设模型有100个系数，但是有10个非零的系数，这就是说，其它90个预测器在预测目标值上是没有用的。L2-norm往往会有非稀疏的系数（non-sparse coefficients），没有这个特点。
**稀疏性（Sparsity）**：这主要是一个向量或矩阵中只有很少的非零（non-zero）条目（entries）。L1-norm有能产生许多零值或非常小的值的系数的属性，很少有大的系数。
**计算效率（Computational efficiency）：**L1-norm没有一个解析解（analytical solution），但是L2-nom有，这使得L2-norm可以被高效的计算。可是，L1-norm的解有稀疏的属性，它可以和稀疏算法一起用，这可以是计算更加高效。

### LDA（线性判别分析）

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4pcfuurj30ma0h678k.jpg" alt="截屏2020-03-04下午7.39.17" style="zoom:50%;" />

将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近.

**分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。**

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4cez36ij30dw03a3yw.jpg" alt="截屏2020-03-04下午7.26.42" style="zoom:50%;" />

通过类内散度矩阵和类间散度矩阵构造广义瑞利商

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4dgf02sj306e03eglq.jpg" alt="截屏2020-03-04下午7.27.53" style="zoom:50%;" />

LDA最大化目标J=类间/类内 

类间尽可能大，类内尽可能小 

然后分子设为1，对分母进行拉格朗日乘子运算求解

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4ksxd2nj308i01k0ss.jpg" alt="截屏2020-03-04下午7.34.58" style="zoom:50%;" />

对于多分类任务则定义全局散度矩阵

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4k8ua5xj30dg066jrs.jpg" alt="截屏2020-03-04下午7.34.22" style="zoom:50%;" />

### SVD

先验概率

对于此类问题大概有一个概率结果，

后验概率

根据当前情况得到的推测

###  PCA主成分分析

PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于**高维数据的降维，可用于提取数据的主要特征分量。**

PCA 的数学推导可以从最大可分型和最近重构性两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，最大可分性

**将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。**

**缓解维度灾难：**PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；
**降噪：**当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；
**过拟合：**PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA **也可能加剧了过拟合**；

**特征独立：**PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；

**优点**：使得数据更易使用，并且可以**去除数据中的噪声**，使得其他机器学习任务更加精确。该算法往往作为预处理步骤，在数据应用到其他算法之前清洗数据。
**缺点**：数据维度降低并不代表特征的减少，因为降维仍旧保留了较大的信息量，**对结果过拟合问题并没有帮助**。不能将降维算法当做解决过拟合问题方法。如果原始数据特征维度并不是很大，也并不需要进行降维。



### 决策树

### 神经网络




### Onehot编码

**分类变量**作为**二进制向量**的表示

要求每个类别之间相互独立，如果之间存在某种连续型的关系

独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。
缺点：当类别的数量很多时，特征空间会变得非常大，成为一个高维稀疏矩阵。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用

四. 什么情况下(不)用独热编码？

用：独热编码用来解决类别型数据的离散值问题;

不用：将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。  Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。



### Normalization（归一化）

数据标准化处理主要包括数据**同趋化**处理和**无量纲**化处理两个方面。

1.把特征的各个维度标准化到特定的区间

2.把有量纲表达式变为无量纲表达式

* 优点
加快基于梯度下降法或随机梯度下降法模型的收敛速度
提升模型的精度

###### 常见标准化方法

* max-min标准化
x*=x-min/max-min
* log
x*=log(x)/log(max)
* atan
要求数据大于0才能映射到[0,1]
* 归一
x*=x/sum(x)
* z-score
   x* = (x - μ ) / σ
 z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。




### 如何解决数据imbalance情况

迄今为止 , 解决不平衡分类问题的策略可以分 为两大类 .一类是从训练集入手 , 通过改变训练集样本分布 ,降低不平衡程度 .另一类是从学习算法入手 , 根据算法在解决不平衡问题时的缺陷 , 适当地修改算法使之适应不平衡分类问题 .平衡训练集的方法主要有**训练集重采样 (re-sampling)方法**和训练集划分方法 .学习算法层面的策略包括分类器集成 、代价敏感学习和特征选择方法等

>数据集处理上

**重采样**
通过增加稀有类训练样本数的上采样 (up-sampling)和减少大类样本数的下采样（down-samplings)使不平衡的样本分布变得比较平衡，从而提高分类器对稀有类的识别率 . 提升模型的泛化能力

* 下采样 
欠采样是通过减少丰富类的大小来平衡数据集，当数据量足够时就该使用此方法。通过保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本，可以检索平衡的新数据集以进一步建模。


* 过采样
最原始的上采样方法是复制稀有类的样本 , 但是这样做容易导致过学习, 并且对提高稀有类识别率没有太大帮助 .较高级的上采样方法则采用一些启发式技巧 , **有选择地复制稀有类样本 , 或者生成新的稀有类样本。**数据量不足时使用过采样通过增加稀有样本的数量来平衡数据集。


**缺陷：**

上采样复制某些稀有类样本 , 或者在它周围生成新的稀有类样本,使得分类器过分注重这些样本,导致过学习.上采样不能从本质上解决稀有类样本的稀缺性和数据表示的不充分性

下采样在去除大类样本的时候 , 容易去除重要的样本信息 .虽然有些启发式的下采样方法 , 只是去除冗余样本和噪声样本,但是多数情况下这类样本只是小部分 ,因此这种方法能够调整的不平衡度相当有限 

数据增强

**One Class Learning**

对于二分类问题，如果正负样本分布比例极不平衡，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。例如：One-class SVM

**阈值移动（权重）**
通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同**

>算法层面

分类器集成


### GBDT

### Xgboost

### 随机森林（分类、回归）

###  深度学习模型层数对实验效果影响



理论上讲越复杂的特征有越强的表征能力。在深度网络中，各个特征会不断的经过线性非线性的综合计算，越深的网络输出表示能力越强的特征。所以，**网络的深度**对于学习表达能力更强的特征至关重要，这一问题在[VGGNet](https://blog.csdn.net/weixin_43624538/article/details/84563093)中得到很好体现

**特征图深度的增加**则使每层输出中可用特征数量的增多。

增加深度带来的首个问题就是**梯度爆炸/消散**的问题

为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu，使用Xaiver初始化等，可以说**梯度消散已经得到了很好的解决**

增加深度的另一个问题就是**网络的degradation**问题，即随着深度的增加，网络的性能会越来越差(网络衰退)，直接体现为在训练集上的准确率会下降，残差网络解决的就是这个问题.

那么，我们退而求其次，已知有网络degradation的情况下，不求加深度能提高准确性，能不能***至少让深度网络实现和浅层网络一样的性能，即让深度网络后面的层至少实现恒等映射的作用***，根据这个想法，作者提出了residual模块来帮助网络实现恒等映射。

### RESNET （深度残差网络）

深度残差网络的设计就是为了克服这种由于网络深度加深而产生的学习效率变低，准确率无法有效提升的问题（也称为**网络退化**）

甚至在一些场景下，网络层数的增加反而会降低正确率。这种本质问题是由于出现了信息丢失而产生的过拟合问题（overfitting，所建的机器学习模型或者是深度学习模型在训练样本中表现的过于优越，导致在验证数据集及测试数据集中表现不佳，即为了得到一致假设而使假设变得过度复杂）。解决思路是尝试着使他们引入这些刺激的差异性和解决泛化能力为主。

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gchxj2e7r2j30ym0hs42f.jpg" alt="截屏2020-03-04下午3.30.55" style="zoom: 25%;" />



### VGG19

### Inception

### MobileNet 

### LSTM GRU




### 强化学习（RL）

### OTSU

otsu使用的是聚类的思想，把图像的灰度数按灰度级分成2个部分，使得两个部分之间的灰度值差异最大，每个部分之间的灰度差异最小，通过方差的计算来寻找一个合适的灰度级别来划分。 所以可以在二值化的时候采用otsu算法来自动选取阈值进行二值化。otsu算法被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响。因此，使类间方差最大的分割意味着错分概率最小。
***
假设图像的背景较暗,并且图像的大小为**M×N**
* 图像中像素的灰度值小于阈值T的像素个数记作**N0**
* 像素灰度大于阈值T的像素个数记作**N1**
* 对于图像I(x,y),前景(即目标)和背景的分割**阈值**记作**T**,
* 前景像素点数占整幅图像的比例记为**ω0**,其平均灰度为**μ0**;
* 背景像素点数占整幅图像的比例记为**ω1**,其平均灰度为**μ1**。
* 图像的总平均灰度记为**μ**。
* 类间方差记为**g**。
* 灰度值（黑色 0：白色 255）
```
ω0=N0/M×N;       (N0+N1=M×N;)
ω1=N1/M×N;       (ω0+ω1=1;)
μ=μ0 * ω0+μ1 * ω1;
g=ω0(μ0-μ)^2 + ω1(μ1-μ)^2;
由上述两式可得 g=ω0ω1(μ0-μ1)^2;
```
采用遍历的方法得到使类间方差最大的阈值T.即为所求。

