#Algorothm

<hr>


* 监督学习 

在拥有特征与正确结果相对应的数据情况下进行学习

* 无监督学习

聚类

### 线性回归 

* 单变量线性回归

基于数据 训练方程参数

构建cost function 计算真实值与预测值误差 得到使方程的代价最小的参数

计算cost 方法 
batch批量梯度下降 计算下降最快的方向 求导 每次进行参数更新 减去学习率*cost的导数 
学习率为下降的步长

* 多变量线性回归

特征为多维  

 处理数据尺度不同 - 特征缩放 能够提高梯度下降速度 椭圆变正圆

正规方程法（最小二乘）
>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的

![截屏2020-03-01下午6.08.03](https://tva1.sinaimg.cn/large/00831rSTly1gcevmq8btoj31820dqn00.jpg)

即：均方误差最小化
单元线性 

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevmb5f6qj30g8078my1.jpg" alt="截屏2020-03-02上午12.07.23" style="zoom:50%;" />

多元线性转为矩阵

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevmkmquqj30fu030aac.jpg" alt="截屏2020-03-02上午12.07.47" style="zoom:50%;" />
**w为（w:b）**
当矩阵为可逆时得到结果

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcevsfav04j309u02qdfx.jpg" alt="截屏2020-03-02上午12.14.02" style="zoom:50%;" />



### 逻辑回归

分类问题 与预测值为离散值
sigmoid function逻辑回归
```1 / (1 + np.exp(-z))```

cost function构建

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcftenhml2j30ey02gwen.jpg" alt="截屏2020-03-02下午7.37.16" style="zoom:50%;" />
其中，<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcftg9fygyj30ev01t3yi.jpg" alt="54249cb51f0086fa6a805291bf2639f1" style="zoom:50%;" />

代入后得到最终代价函数

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcfthqgarqj30sy02m0t3.jpg" alt="截屏2020-03-02下午7.39.55" style="zoom:50%;" />
然后进行梯度下降算法求θ

逻辑回归的代价函数看起来和线性回归一样  但是 h(theta) 不同 即假设函数。线性回归多为多项式

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgt7vb51lj307i022weh.jpg" alt="截屏2020-03-03下午4.15.28" style="zoom:50%;" />



g为sigmoid X为特征向量 theta参数

一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。有：**共轭梯度**（**Conjugate Gradient**），**局部优化法**(**Broyden fletcher goldfarb shann,BFGS**)和**有限内存局部优化法**(**LBFGS**) ，**fminunc**



**共轭梯度**

**局部优化法**

**有限内存局部优化法**





### 正则化

处理过拟合问题

保留所有的特征，但是减少参数的大小
高次项导致了过拟合的产生，降低高次项系数
**cost function**

引入正则化参数lamda
* 正则化线性回归

**梯度下降方法**
<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgqzagzrhj30iq03caab.jpg" alt="截屏2020-03-03下午2.58.04" style="zoom:50%;" />

求导整理后得



<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgrt7hxlxj30k202ujrm.jpg" alt="截屏2020-03-03下午3.27.40" style="zoom:50%;" />

**正规方程**

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgrul7l50j308c0230so.jpg" alt="71d723ddb5863c943fcd4e6951114ee3" style="zoom:50%;" />


* 正则化逻辑回归

  cost function

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgsfdfui8j30xc03wmxn.jpg" alt="截屏2020-03-03下午3.48.50" style="zoom:50%;" />



* L1正则化：Lasso回归
  产生**稀疏权值矩阵**，即产生一个稀疏模型，可以用于特征选择
  权值向量w中各个元素的绝对值之和

  <img src="https://tva1.sinaimg.cn/large/00831rSTly1gcgv4zgwcuj30ay03yaa3.jpg" alt="截屏2020-03-03下午5.22.29" style="zoom:50%;" />

令L=正则项，此时我们的任务变成在L约束下求出J0取最小值的解。

因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），J0与这些角接触的机率会远大于与LLL其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch1u29ijdj308u089js0.jpg" alt="20160904184428459" style="zoom:50%;" />



正则参数lamda 越大，越容易在x=0处取得最小值


* L2正则化：Ridge回归
可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合
权值向量w中各个元素的平方和然后再求平方根,L2损失函数：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch1uytqgtj309c032glm.jpg" alt="截屏2020-03-03下午9.15.12" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch20l6e65j308u089q3l.jpg" alt="20160904184646963" style="zoom:50%;" />

拟合过程中通常都倾向于让权值尽可能小，适应不同的数据集，也在一定程度上避免了过拟合现象，抗扰动能力强

L2正则化的梯度下降公示：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gch24ehzh7j30mc03u3yv.jpg" alt="截屏2020-03-03下午9.23.52" style="zoom:50%;" />

比正常回归的多的部分（其中lamda为正则系数）

正则参数lamda 越大，theta 衰减越快，求costfunction最值时参数也会越小

#### L1、L2比较分析

L1-norm(范数)，也叫作最小绝对偏差（leastabsolute deviations, LAD），最小绝对误差（least absolute errors，LAE）.它是最小化目标值yi和估计值f(xi)绝对差值的求和。

L2-norm(范数)也称为最小均方(least squares)，它是最小化目标值yi和估计值f(xi)平方和。

**鲁棒性（Robustness）：**最小绝对值偏差的方法应用领域很广，相比最小均方的方法，它的鲁棒性更好，LAD能对数据中的异常点有很好的**抗干扰能力**，异常点可以安全的和高效的忽略，这对研究帮助很大。
如果异常值对研究很重要，最小均方误差则是更好的选择。

对于L2-norm，由于是均方误差，如果误差>1的话，那么平方后，相比L-norm而言，误差就会被放大很多。因此模型会对样例更敏感。如果样例是一个异常值，模型会调整最小化异常值的情况，以牺牲其它更一般样例为代价，因为相比单个异常样例，那些一般的样例会得到更小的损失误差。

**稳定性：**LAD方法的不稳定属性意思是，对于一个书评调整的数据集，回归线可能会跳跃很大

![20170920234145310](https://tva1.sinaimg.cn/large/00831rSTly1gch65880lnj30o109k76i.jpg)

这里的稳定性指的是对于 特征点的微调所导致的回归线的变动，微调之后 以及大调之后回归变化，所以l1比l2在此方面更具有稳定性。

**解决方案唯一性** 对于一个问题的手链方式l2只有一种最短，l1路径长度相同情况下手链方式有可能不同

![20170920234250809](https://tva1.sinaimg.cn/large/00831rSTly1gch69p8ar6j30ao0asq46.jpg)

**内置的特征选择（Built-in feature selection）**：这是L1-norm经常被提及的一个优点，而L2-norm没有。这实际上是L1-norm的一个结果，L1-norm往往会使系数变得**稀疏**（sparse coefficients），易于进行特征选择。假设模型有100个系数，但是有10个非零的系数，这就是说，其它90个预测器在预测目标值上是没有用的。L2-norm往往会有非稀疏的系数（non-sparse coefficients），没有这个特点。
**稀疏性（Sparsity）**：这主要是一个向量或矩阵中只有很少的非零（non-zero）条目（entries）。L1-norm有能产生许多零值或非常小的值的系数的属性，很少有大的系数。
**计算效率（Computational efficiency）：**L1-norm没有一个解析解（analytical solution），但是L2-nom有，这使得L2-norm可以被高效的计算。可是，L1-norm的解有稀疏的属性，它可以和稀疏算法一起用，这可以是计算更加高效。

### LDA（线性判别分析）

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4pcfuurj30ma0h678k.jpg" alt="截屏2020-03-04下午7.39.17" style="zoom:50%;" />

将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近.

**分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。**

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4cez36ij30dw03a3yw.jpg" alt="截屏2020-03-04下午7.26.42" style="zoom:50%;" />

通过类内散度矩阵和类间散度矩阵构造广义瑞利商

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4dgf02sj306e03eglq.jpg" alt="截屏2020-03-04下午7.27.53" style="zoom:50%;" />

LDA最大化目标J=类间/类内 

类间尽可能大，类内尽可能小 

然后分子设为1，对分母进行拉格朗日乘子运算求解

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4ksxd2nj308i01k0ss.jpg" alt="截屏2020-03-04下午7.34.58" style="zoom:50%;" />

对于多分类任务则定义全局散度矩阵

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gci4k8ua5xj30dg066jrs.jpg" alt="截屏2020-03-04下午7.34.22" style="zoom:50%;" />

### SVD

先验概率

对于此类问题大概有一个概率结果，

后验概率

根据当前情况得到的推测

###  PCA主成分分析

PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于**高维数据的降维，可用于提取数据的主要特征分量。**

PCA 的数学推导可以从最大可分型和最近重构性两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，最大可分性

**将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。**

**缓解维度灾难：**PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；
**降噪：**当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；
**过拟合：**PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA **也可能加剧了过拟合**；

**特征独立：**PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；

**优点**：使得数据更易使用，并且可以**去除数据中的噪声**，使得其他机器学习任务更加精确。该算法往往作为预处理步骤，在数据应用到其他算法之前清洗数据。
**缺点**：数据维度降低并不代表特征的减少，因为降维仍旧保留了较大的信息量，**对结果过拟合问题并没有帮助**。不能将降维算法当做解决过拟合问题方法。如果原始数据特征维度并不是很大，也并不需要进行降维。



### 决策树

### 神经网络

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmj3jaeqxj30de089dgo.jpg" alt="8293711e1d23414d0a03f6878f5a2d91" style="zoom: 67%;" />

其中x1,x2,x3 是输入单元（input units），我们将原始数据输入给它们。a1 ,a2 ,a3 是中间单元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算Htheta(x)。对于上图所示的模型，激活单元和输出分别表达为：

![截屏2020-03-08下午3.00.38](https://tva1.sinaimg.cn/large/00831rSTly1gcmj4mge49j316403uwfd.jpg)

我们可以知道：每一个都是由上一层所有的和每一个所对应的决定的。（我们把这样从左到右的算法称为**前向传播算法( FORWARD PROPAGATION )**



神经网络就像是**logistic regression**，只不过我们把**logistic regression**中的输入向量[x1,x2,x3]变成了中间层的[a1,a2,a3],我们可以把看成更为高级的特征值，也就是的进化体,能更好的预测新数据,这就是神经网络相比于逻辑回归和线性回归的优势。

不同的权重能表示不同的运算



**cost function**

![截屏2020-03-08下午3.18.02](https://tva1.sinaimg.cn/large/00831rSTly1gcmjn22hcqj316w0343zb.jpg)

通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出K个预测，基本上我们可以利用循环，对每一行特征都预测K个不同结果，然后在利用循环在K个预测中选择可能性最高的一个，将其与y中的实际数据进行比较



**反向传播（Backpropagation Algorithm）**

为了计算代价函数的偏导数，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。

**例子：**



<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmvoj83tej30q40kgq7b.jpg" alt="853467-20160630142019140-402363317" style="zoom: 33%;" />

通过一个实际例子推导反向传播更新网络权重的过程。

* 前向传播：计算各节点数值

  **权值和：**

  net(h1) =w1 * i1+w2 * i2 + b1*1

  net(h2) = w3 * i1+w4 * i2+ b1*1

  **激活函数计算（激活值）：**作为下一层的输入

  out(h1) = sigmoid(net(h1))

  out(h2) = sigmoid(net(h2))

  **隐藏层权值和：**

  net(o1)=w5 * out(h1) + w6 * out(h2) +b2 * 1

  net(o2)=w7 * out(h1) + w8 * out(h2) + b2 * 1

  **激活输出：**

  out(o1) = sigmoid(net(o1))

  out(o2) = sigmoid(net(o2))

* 反向传播

  总误差计算：

  <img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmw8wzjodj30dg02ugln.jpg" alt="853467-20160630151201812-1014280864" style="zoom:50%;" />

  E(total)=E(o1)+E(o2)//target为真实值

  权值更新：

  以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）

  <img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmwmzzme5j30t80e8acr.jpg" alt="853467-20160630152018906-1524325812" style="zoom:50%;" />



分别计算每一个式子

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmx5zkopjj30u20b8tam.jpg" alt="截屏2020-03-08下午11.06.11" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmx72grwgj30ue08m0tr.jpg" alt="截屏2020-03-08下午11.07.28" style="zoom:50%;" />

sigmoid求导

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmx8fbtt6j30ny08udgs.jpg" alt="截屏2020-03-08下午11.08.32" style="zoom:50%;" />

也可以将前两项用dlert代替

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmxd9228zj30hy02c0sr.jpg" alt="853467-20160630153405296-436656179" style="zoom:50%;" />

w5权重更新

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcmxe913bsj30ti02e0t0.jpg" alt="853467-20160630153614374-1624035276" style="zoom: 50%;" />

学习率这里取得0.5

**隐含层权重更新**








### Onehot编码

**分类变量**作为**二进制向量**的表示

**优点：**
要求每个类别之间相互独立，能够处理**非连续**型数值特征，也就是离散值。

独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了**扩充特征**的作用。它的值只有0和1，不同的类型存储在垂直的空间。

将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。（欧式距离）

**缺点：**
如果原本的标签编码是有序的，那one hot编码就不合适了——会丢失顺序信息。

如果特征的特征值数目特别多，特征向量就会非常大，且非常稀疏。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用





### Normalization（归一化）

数据标准化处理主要包括数据**同趋化**处理和**无量纲**化处理两个方面。

1.把特征的各个维度标准化到特定的区间

2.把有量纲表达式变为无量纲表达式

* 优点
加快基于梯度下降法或随机梯度下降法模型的收敛速度
提升模型的精度

###### 常见标准化方法

* max-min标准化
x*=x-min/max-min
* log
x*=log(x)/log(max)
* atan
要求数据大于0才能映射到[0,1]
* 归一
x*=x/sum(x)
* z-score
   x* = (x - μ ) / σ
 z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。




### 如何解决数据imbalance情况

迄今为止 , 解决不平衡分类问题的策略可以分 为两大类 .一类是从训练集入手 , 通过改变训练集样本分布 ,降低不平衡程度 .另一类是从学习算法入手 , 根据算法在解决不平衡问题时的缺陷 , 适当地修改算法使之适应不平衡分类问题 .平衡训练集的方法主要有**训练集重采样 (re-sampling)方法**和训练集划分方法 .学习算法层面的策略包括分类器集成 、代价敏感学习和特征选择方法等

>数据集处理上

**重采样**
通过增加稀有类训练样本数的上采样 (up-sampling)和减少大类样本数的下采样（down-samplings)使不平衡的样本分布变得比较平衡，从而提高分类器对稀有类的识别率 . 提升模型的泛化能力

* 下采样 
  欠采样是通过减少丰富类的大小来平衡数据集，当数据量足够时就该使用此方法。通过保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本，可以检索平衡的新数据集以进一步建模。

  EasyEnsemble，利用模型融合的方法

  BalanceCascade，利用增量训练的思想（Boosting）

  NearMiss,利用KNN试图挑选那些最具代表性的大众样本


* 过采样
最原始的上采样方法是复制稀有类的样本 , 但是这样做容易导致过学习, 并且对提高稀有类识别率没有太大帮助 .较高级的上采样方法则采用一些启发式技巧 , **有选择地复制稀有类样本 , 或者生成新的稀有类样本。**数据量不足时使用过采样通过增加稀有样本的数量来平衡数据集。


**缺陷：**

上采样复制某些稀有类样本 , 或者在它周围生成新的稀有类样本,使得分类器过分注重这些样本,导致过学习.上采样不能从本质上解决稀有类样本的稀缺性和数据表示的不充分性

下采样在去除大类样本的时候 , 容易去除重要的样本信息 .虽然有些启发式的下采样方法 , 只是去除冗余样本和噪声样本,但是多数情况下这类样本只是小部分 ,因此这种方法能够调整的不平衡度相当有限 

数据增强

**One Class Learning**

对于二分类问题，如果正负样本分布比例极不平衡，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。例如：

One-class SVM

> SVDD

采用一个超球体而不是一个超平面来做划分，该算法在特征空间中获得数据周围的球形边界，期望最小化这个超球体的体积，从而最小化异常点数据的影响。

假设产生的超球体参数为中心 o 和对应的超球体半径 r >0，超球体体积V(r) 被最小化，中心 o 是支持行了的线性组合；跟传统SVM方法相似，可以要求所有训练数据点xi到中心的距离严格小于r。但是同时构造一个惩罚系数为 C 的松弛变量 ζi ,优化问题入下所示

**采用拉格朗日对偶求解之后，可以判断新的数据点 z 是否在内，如果 z 到中心的距离小于或者等于半径 r ，则不是异常点，如果在超球体以外，则是异常点**

Robust covariance 

Isolation Forest

**数据合成**
SMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。

**阈值移动（权重）**
通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同**

>算法层面

分类器集成
一个很好的方法去处理非平衡数据问题，并且在理论上证明了。这个方法便是由Robert E. Schapire于1990年在Machine Learning提出的”The strength of weak learnability” ，该方法是一个boosting算法，它递归地训练三个弱学习器，然后将这三个弱学习器结合起形成一个强的学习器。我们可以使用这个算法的第一步去解决数据不平衡问题。 
  首先使用原始数据集训练第一个学习器L1。 
  然后使用50%在L1学习正确和50%学习错误的的那些样本训练得到学习器L2，即从L1中学习错误的样本集与学习正确的样本集中，循环一边采样一个。 
  接着，使用L1与L2不一致的那些样本去训练得到学习器L3。 
  最后，使用投票方式作为最后输出。 
  那么如何使用该算法来解决类别不平衡问题呢？ 
  假设是一个二分类问题，大部分的样本都是true类。让L1输出始终为true。使用50%在L1分类正确的与50%分类错误的样本训练得到L2，即从L1中学习错误的样本集与学习正确的样本集中，循环一边采样一个。因此，L2的训练样本是平衡的。L使用L1与L2分类不一致的那些样本训练得到L3，即在L2中分类为false的那些样本。最后，结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则true。

### SVM

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

**非线性，是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现，这一部分是SVM的精髓**



在不同类别的样本间，找到一个超平面从而实现对样本进行划分；同时此超平面与类别间的间隔要最大；

第一个问题：构造**最大间隔函数**，即不同直线间的距离 L=g(x)/||w|| ,g(x)=wx+b;w为其法向量，**g(X)为以w为法向量的一簇超平面，在二维下就是一簇直线。**





线性规划（凸规划）

几何间隔：![clip_image002_2](http://www.blogjava.net/images/blogjava_net/zhenandaci/WindowsLiveWriter/SVMPart1_EEC8/clip_image002_2.gif)

我们要使**间隔最大**，则在x固定情况下要使w取最小，其中1/2||w||方  为我们需要求的一个最优化即min，系数加上了1/2只是为了求导时计算方便，即：min 1/2||w||方，

**约束条件**，如不加约束，就只是间隔达到最大但是不能正确的分类，即要在能够正确将不同类别样本分开的情况下间隔最大：约束条件：|g(x)|>=1,即|y(wx+b)|>=1=>y(wx+b)-1>=0,（这里使用二次规划为例）

把分类问题归结为凸二次规划问题，即可对其进行构造拉格朗日乘子运算求最优解

即

![clip_image002[5]](http://www.blogjava.net/images/blogjava_net/zhenandaci/WindowsLiveWriter/SVMPart2_1603/clip_image002%5B5%5D.gif)

转化为求参数w和b的过程，w=α1x1+α2x2+…+αnxn b可有w推导出来，所以w为我们的变量，我们把w写成：

![clip_image002_2](http://www.blogjava.net/images/blogjava_net/zhenandaci/WindowsLiveWriter/SVM_1244A/clip_image002_2.gif)

如果我们的数据线性不可分，那么可将它映射到高维空间中实现线性可分，即使其在高维空间中更接近最优解的范围，这个时候就用到了核函数，



* 核函数

  

径向基核函数（RBF）



它能够把原始特征映射到无穷维。  

高斯核函数(Gaussian Kernel)

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcq7ilwmp0j30h802s0sw.jpg" alt="截屏2020-03-11下午7.20.48" style="zoom:50%;" />

如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。支持向量机也可以不使用核函数，不使用核函数又称为**线性核函数**(**linear kernel**)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。

核函数的基本作用就是接受两个低维空间里的向量，能够计算出经过某个变换后在高维空间里的向量内积值。







* 松弛变量

因为松弛变量是非负的，因此最终的结果是要求间隔可以比1小。但是**当某些点出现这种间隔比1小的情况时（这些点也叫离群点）**，意味着我们**放弃了对这些点的精确分类**，而这对我们的分类器来说是种损失。但是放弃这些点也带来了好处，那就是使分类面不必向这些点的方向移动，因而可以得到更大的几何间隔（在低维空间看来，分类边界也更平滑）。显然我们必须权衡这种损失和好处。好处很明显，我们得到的分类间隔越大，好处就越多

优化问题在解的过程中，C一直是定值，要记住。
**C的调整其实是在调整函数对与离群点的重视程度**



发生数据偏斜时，函数分割界限会向少的一方倾斜，会想也可以通过对不同的类别赋予不同的惩罚因子

这个地方可能就是和权重有点类似



**多分类问题**

我们可以通过训练多个分类器实现多分类的效果，即目标类别与其余类别

下面是支持向量机的两个参数和的影响：
C=1/lamda 
C较大时，相当于较小，可能会导致过拟合，高方差；

C较小时，相当于较大，可能会导致低拟合，高偏差；

delta较大时，可能会导致低方差，高偏差；

delta较小时，可能会导致低偏差，高方差。
核函数举例
多项式核函数（Polynomial Kernel）
字符串核函数（String kernel）
卡方核函数（ chi-square kernel）
直方图交集核函数（histogram intersection kernel）

n为特征数，m为训练样本数。

(1)如果相较于m而言，n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。

(2)如果n较小，而且m大小中等，例如n在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。

(3)如果n较小，而m较大，例如n在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。

值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。

对付数据集偏斜问题的方法之一就是在惩罚因子上作文章，想必大家也猜到了，那就是给样本数量少的负类更大的惩罚因子，表示我们重视这部分样本，libSVM这个算法包在解决偏斜问题的时候用的就是这种方法

### GBDT

### 集成学习
bagging boosting
* Xgboost

算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值.

### 随机森林（bagging）

* 决策树

构造随机森林的4个步骤

假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
按照步骤1~3建立大量的决策树，这样就构成了随机森林了。


随机森林是由很多决策树构成的，不同决策树之间没有关联。
当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。


优点

它可以出来很高维度（特征很多）的数据，并且不用降维，无需做特征选择
它可以判断特征的重要程度
可以判断出不同特征之间的相互影响
不容易过拟合
训练速度比较快，容易做成并行方法
实现起来比较简单
对于不平衡的数据集来说，它可以平衡误差。
如果有很大一部分的特征遗失，仍可以维持准确度。
缺点

随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的

###  深度学习模型层数对实验效果影响



理论上讲越复杂的特征有越强的表征能力。在深度网络中，各个特征会不断的经过线性非线性的综合计算，越深的网络输出表示能力越强的特征。所以，**网络的深度**对于学习表达能力更强的特征至关重要，这一问题在[VGGNet](https://blog.csdn.net/weixin_43624538/article/details/84563093)中得到很好体现

**特征图深度的增加**则使每层输出中可用特征数量的增多。

增加深度带来的首个问题就是**梯度爆炸/消散**的问题

为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu，使用Xaiver初始化等，可以说**梯度消散已经得到了很好的解决**

增加深度的另一个问题就是**网络的degradation**问题，即随着深度的增加，网络的性能会越来越差(网络衰退)，直接体现为在训练集上的准确率会下降，残差网络解决的就是这个问题.

那么，我们退而求其次，已知有网络degradation的情况下，不求加深度能提高准确性，能不能***至少让深度网络实现和浅层网络一样的性能，即让深度网络后面的层至少实现恒等映射的作用***，根据这个想法，作者提出了residual模块来帮助网络实现恒等映射。

### RESNET （深度残差网络）

深度残差网络的设计就是为了克服这种由于网络深度加深而产生的学习效率变低，准确率无法有效提升的问题（也称为**网络退化**）

甚至在一些场景下，网络层数的增加反而会降低正确率。这种本质问题是由于出现了信息丢失而产生的过拟合问题（overfitting，所建的机器学习模型或者是深度学习模型在训练样本中表现的过于优越，导致在验证数据集及测试数据集中表现不佳，即为了得到一致假设而使假设变得过度复杂）。解决思路是尝试着使他们引入这些刺激的差异性和解决泛化能力为主。

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gchxj2e7r2j30ym0hs42f.jpg" alt="截屏2020-03-04下午3.30.55" style="zoom: 25%;" />





![1436741-20181021163605989-1397336088](https://tva1.sinaimg.cn/large/00831rSTly1gcjg867k63j31n90pl45w.jpg)

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gcjgamqviwj30qy09kwfj.jpg" alt="1436741-20181021163243972-2143143177" style="zoom:50%;" />



左边building block 右边bottleneck block对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量







### VGG19



### Inception

### MobileNet 

### LSTM GRU



### 强化学习（RL）



### SIFT





### 评估校验等

对实验的调整操作

1. 获得更多的训练样本——通常是有效的，但代价较大
2. 特征的数量
3. 正则化程度lamda

测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：

对于线性回归模型，我们利用测试集数据计算代价函数
对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：
误分类的比率，对于每一个测试集样本，计算：然后对计算结果求平均

交叉验证（即：数据分为训练、交叉验证、测试）

模型选择的方法为：
1.使用训练集训练出10个模型
2.用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）
3.选取代价函数值最小的模型
4.用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）

训练集误差和交叉验证集误差近似时：偏差/欠拟合 

交叉验证集误差远大于训练集误差时：方差/过拟合

* 过拟合
得更多的训练样本——解决高方差
尝试减少特征的数量——解决高方差
尝试增加正则化程度λ——解决高方差
* 欠拟合
尝试获得更多的特征——解决高偏差
尝试增加多项式特征——解决高偏差
尝试减少正则化程度λ——解决高偏差

**查准率（Precision）和查全率（Recall）**我们将算法预测的结果分成四种情况：

正确肯定（True Positive,TP）：预测为真，实际为真
正确否定（True Negative,TN）：预测为假，实际为假 
错误肯定（False Positive,FP）：预测为真，实际为假 
错误否定（False Negative,FN）：预测为假，实际为真

**查准率=TP/(TP+FP)**，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 
**查全率=TP/(TP+FN)**，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。

**F1_score=2(PR)/(P+R)**

**confusion matrix**

### OTSU

otsu使用的是聚类的思想，把图像的灰度数按灰度级分成2个部分，使得两个部分之间的灰度值差异最大，每个部分之间的灰度差异最小，通过方差的计算来寻找一个合适的灰度级别来划分。 所以可以在二值化的时候采用otsu算法来自动选取阈值进行二值化。otsu算法被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响。因此，使类间方差最大的分割意味着错分概率最小。
***
假设图像的背景较暗,并且图像的大小为**M×N**
* 图像中像素的灰度值小于阈值T的像素个数记作**N0**
* 像素灰度大于阈值T的像素个数记作**N1**
* 对于图像I(x,y),前景(即目标)和背景的分割**阈值**记作**T**,
* 前景像素点数占整幅图像的比例记为**ω0**,其平均灰度为**μ0**;
* 背景像素点数占整幅图像的比例记为**ω1**,其平均灰度为**μ1**。
* 图像的总平均灰度记为**μ**。
* 类间方差记为**g**。
* 灰度值（黑色 0：白色 255）
```
ω0=N0/M×N;       (N0+N1=M×N;)
ω1=N1/M×N;       (ω0+ω1=1;)
μ=μ0 * ω0+μ1 * ω1;
g=ω0(μ0-μ)^2 + ω1(μ1-μ)^2;
由上述两式可得 g=ω0ω1(μ0-μ1)^2;
```
采用遍历的方法得到使类间方差最大的阈值T.即为所求。



